#! /usr/bin/env python3

import os, sys, traceback
import cmd
import readline
import atexit
import re
import argparse
from collections import OrderedDict
import functools
import numpy as np
import torch
import torch.nn as nn
from torchvision import transforms, datasets
from torch.nn.functional import softmax
from PIL import Image
from pm_base import ShellBase
from pm_helper_classes import Dataset
from layer_visualizer import LayerVisualizer
from pm_commands import Commands
from llm_service import LLMService


model = None
image = None

class Config:
    pass 

class Shell( ShellBase, Commands, cmd.Cmd ):
    def __init__( self, config ):
        super().__init__()
        self.config = config
        self.rc_lines = []
        self.dataset = None
        self.compare = None
        self.data_post_process_fn = None
        #self.use_rawinput = False
        self.cur_frame = sys._getframe().f_back

        self.load_and_execute_rc_file()
        self.init_history( histfile=".pmdebug_history" )
        atexit.register( self.save_history, histfile=".pmdebug_history" )

        if hasattr( config, 'dataset' ) and config.dataset:
            self.do_set_dataset( config.dataset )
            self.load_first_image_from_dataset()

        # Initialize LLM service
        self.message( "Initializing LLM service...", end="" )
        try:
            # Use path relative to this script's location
            script_dir = os.path.dirname( os.path.abspath( __file__ ) )
            service_config_file = os.path.join( script_dir, "llm_service_config.yaml" )
            self.llm_service = LLMService( service_config_file )
            self.llm_service.create_service()
        except Exception as e:
            self.llm_service = None
            print( f"WARNING: LLM service initialization failed with {e}" )
        self.message( "Done" )


    ##############################################
    # Functions overridden from base class go here
    ##############################################
    def precmd( self, line ):
        if line.find( " " ) > 0:
            args = line.split( " " )
            map( str.strip, args )
            c = "{}_{}".format( args.pop( 0 ), args.pop( 0 ) )
            if ( callable( getattr( self, "do_" + c, None ) ) ):
                line = "{} {}".format( c, " ".join( args ) )        
        return line


    def default( self, line ):
        if line[ :1 ] == '!':
            line  = line[ 1: ]

        is_assign = False
        if line.find( '=' ) > 0:
            var, _ = line.split( '=', maxsplit=1 )
            var = var.strip()
            is_assign = True

        locals = self.cur_frame.f_locals
        globals = self.cur_frame.f_globals
        
        try:
            code = compile( line + "\n", "<stdin>", "single" )
            saved_stdin = sys.stdin
            saved_stdout = sys.stdout
            sys.stdin = self.stdin
            sys.stdout = self.stdout

            try:
                exec( code, globals, locals )
            finally:
                sys.stdin = saved_stdin
                sys.stdout = saved_stdout
        except:
            exec_info = sys.exc_info()[ :2 ]
            self.error( traceback.format_exception_only( *exec_info )[ -1 ].strip() )
        else:
            if is_assign and var and var in self.models:
                self.message( "Resyncing model \"{}\"".format( var ) )
                self.resync_model( var )


    ####################################
    # Decorators
    ####################################
    def supports_compare( func ):    # pylint: disable=no-self-argument
        """We store self.compare on the stack to make the decorator reentrant
        This avoids recursion in case a function that supports this decorator
        is called again inside the decorator
        """

        @functools.wraps( func )
        def wrapper( self, *kargs, **kwargs ):
            compare = self.compare
            self.stack.append( self.compare )
            self.compare = None

            if compare:
                self.fig.set_mode( "dual" )
    
            try:
                if compare == "image":
                    self.do_show_image( *kargs, **kwargs )
                elif compare is not None:
                    func( self, compare )       #pylint: disable=not-callable

                func( self, *kargs, **kwargs )  #pylint: disable=not-callable
            except:
                raise
            finally:
                self.compare = self.stack.pop()
                if compare:
                    self.fig.set_mode( "single" )

        return wrapper


    ########################################################
    # do_* command functions go here
    # Commands that use global variables or use decorators
    # should be defined below.
    # Rest of the commands can go in Commands class
    ########################################################
    def do_load_image( self, args ):
        """Load a single image from the path specified
        Usage: load image [ path ]
        """
        global image

        image_path = os.path.join( self.config.image_path, args )
        if not os.path.isfile( image_path ):
            self.error( "Image not found")
            return
        self.message( "Loading image {}".format( image_path ) )
        image = Image.open( image_path )
        transform = transforms.Compose( [ transforms.Resize( ( self.image_size, self.image_size ) ),
                                         
                                          transforms.ToTensor() ] )
        image = transform( image ).float().unsqueeze( 0 )


    def do_image_next( self, args ):
        """Load the next available image from a dataset:
        Usage: image next
        
        This command operates on a dataset. A dataset must be configued for this 
        command. If there are no more images available to be loaded, it keeps the 
        last available image.
        The "image" global variable points to the loaded image.
        """ 
        global image
        if self.dataset is None:
            self.message( "Please configure a dataset first" )
            return

        self.dataset.next()
        image = self.dataset.load()
        self.fig.imshow( image )


    def do_load_checkpoint( self, args ):
        """Load a checkpoint file into the model:
        Usage: load checkpoint [ filename ]

        If no file is specified, checkpoint_name specified in the
        config file is used"""
        model_info = self.cur_model
        if model_info is None:
            self.error( "No default model set in context." )
            return
        
        model = model_info.model

        if args:
            file = os.path.join( self.config.checkpoint_path, args )
        else:
            file = os.path.join( self.config.checkpoint_path, self.config.checkpoint_name )
        if not os.path.isfile( file ):
            self.error( "Checkpoint file not found" )
            return

        chkpoint = torch.load( file, map_location="cpu" )
        self.message( "Model \"{}\", loading checkpoint: {}".format( model_info.name, file ) )
        
        state_dict = chkpoint[ "model" ]

        try:
            model.load_state_dict( state_dict )
        except RuntimeError:
            new_state_dict = OrderedDict( [ ( k[ 7: ], v ) for k, v in state_dict.items() 
                                                            if k.startswith( "module" ) ] )
            model.load_state_dict( new_state_dict )

    do_load_chkp = do_load_checkpoint
    do_laod_chkp = do_load_checkpoint


    @supports_compare
    def do_infer_image( self, args ):
        """Run inference on image:
        Usage: infer image [ model_name ]

        If an optional "model_name" is provided, inference is run
        on that model. The "model_name" must be present in context.
        If no "model_name" is provided, inference is run on the current 
        model set in the context.

        Input image is taken from the global "image" variable.
        """
        model_info, _ = self.get_info_from_context( args )
        if model_info is None:
            return

        img = self.load_from_global( "image" )
        if img is None:
            self.error( "Please load an input image first" )
            return

        net = model_info.model
        out = net( image )
        probs, idxs = softmax( out, dim=1 ).topk( 5, dim=1 )
        self.message( "{}:".format( model_info.name ) )
        for idx, prob in zip( idxs[ 0 ], probs[ 0 ] ):
            self.message( "{:<10}{:4.2f}".format( idx.data, prob.data * 100 ) )

    do_infer = do_infer_image
    do_show_infer = do_infer_image


    @supports_compare
    def do_show_first_layer_weights( self, args ):
        """Display the weight vectors in a layer as a grid:
        Usage: show first layer weights [ model name/tensor ]
        
        If no arguments are given, it displays the first Conv layer weights in the default model
        set in the context
        If an optional model name if provided, it displays the first layer weight for that
        model
        Optionally, a weight tensor can be given as an argument.
        """
        if not args and not self.cur_model:
            self.error( "No default model is set. Please set a model in context first." )
            return
        
        title = "Tensor {}".format( args )
        _weight = None
        if args and args not in self.models:
            _weight = self.in_place_eval( args )
            if _weight is None or not isinstance( _weight, torch.Tensor ):
                self.error( "Can not display \"{}\". Not a model in context, or a tensor.".format( args ) )
                return

            if _weight.dim() != 4 and _weight.size[ 1 ] not in ( 3, 1 ):
                self.error( "Tensor \"{}\" is incorrect shape for display".format( args ) )
                return

        if _weight is None: 
            model_info = self.models[ args ] if args else self.cur_model
            title = "{} first layer weights".format( model_info.name )
            _, conv = model_info.find_first_instance( type=nn.Conv2d )
            if not conv:
                self.error( "No Conv2d layer found" )
                return
            _weight = conv.weight.detach()
        
        self.show_weights_as_grid( _weight, title )

    do_show_flw = do_show_first_layer_weights


    @supports_compare
    def do_show_activations( self, args ):
        model_info, layer_info = self.get_info_from_context( args )

        if model_info is None:
            return

        img = self.load_from_global( "image" )
        if img is None:
            self.error( "Please load an input image first" )
            return

        if self.data_post_process_fn:
            self.message( "Using processing function {}".format( self.data_post_process_fn.__name__ ) )

        layer_info.register_forward_hook()
        net = model_info.model
        out = net( image )
        self.message( "{} out: {}".format( model_info.name, out.argmax() ) )

        title = "{}{} activations".format( model_info.name, layer_info.id )
        self.display_bargraph( layer_info.data(), title, reduce_fn=self.data_post_process_fn )

    do_show_act = do_show_activations


    def do_activations_next( self, args ):
        global image
        
        if self.dataset is None:
            self.error( "No dataset configured" )
            return

        self.dataset.next()
        image = self.dataset.load()
        self.do_show_activations( args=args )

    do_act_next = do_activations_next


    @supports_compare
    def do_show_heatmap( self, args ):
        model_info, layer_info = self.get_info_from_context( args )
        
        if model_info is None:
            self.message( "Please set a model in context first" )
            return

        img = self.load_from_global( "image" )
        if img is None:
            self.error( "No input image available" )
            return

        id, layer = model_info.find_last_instance( layer=nn.Conv2d )
        layer_info = model_info.get_layer_info( id, layer )
        layer_info.register_forward_hook()

        net = model_info.model
        idx = net( image ).argmax()
        
        _, fc = model_info.find_last_instance( layer=nn.Linear )
        fc_weights = fc.weight[ idx ].data.numpy()

        activations = layer_info.data()[ 0 ].data.numpy()        
        nc, h, w = activations.shape
        
        cam = fc_weights.reshape( 1, nc ).dot( activations.reshape( nc, h * w ) )
        cam = cam.reshape( h, w )
        cam = ( cam - np.min( cam ) ) / np.max( cam )
        cam = Image.fromarray( cam )

        _, _, h, w = img.size()     
        cam = cam.resize( ( h, w ), Image.BICUBIC )
        cam = transforms.ToTensor()( cam )[ 0 ]
        
        msg = "{} guess: {}".format( model_info.name, idx )
        self.message( msg )
        window = self.fig.get_or_create_window()
        window.add_title( msg )
        window.add_image( img )
        window.add_image( cam, cmap="jet", alpha=0.5 )
        window.show()

    do_show_heat = do_show_heatmap


    def do_heatmap_next( self, args ):
        global image
        
        if self.dataset is None:
            self.error( "No dataset configured" )
            return

        self.dataset.next()
        image = self.dataset.load()
        self.do_show_heatmap( args=args )

    do_heat_next = do_heatmap_next


    @supports_compare
    def do_show_weights( self, args ):
        model_info, layer_info = self.get_info_from_context( args )
        if model_info is None:
            return

        id, layer = layer_info.id, layer_info.layer
        self.message( "Current layer is {}: {}".format( id, layer ) )

        if self.data_post_process_fn:
            self.message( "Post processing function is {}".format( self.data_post_process_fn.__name__ ) )

        try:
            data = layer_info.layer.weight.unsqueeze( 0 )
        except:
            self.error( "Current layer has no weights")
        else:
            title = "{} weights".format( layer_info.id )
            self.display_bargraph( data, title, reduce_fn=self.data_post_process_fn )

    do_show_weight = do_show_weights
    do_show_wei = do_show_weights


    @supports_compare
    def do_show_grads( self, args ):
        model_info, layer_info = self.get_info_from_context( args )
        if model_info is None:
            return
    
        id, layer = layer_info.id, layer_info.layer
        self.message( "Current layer is {}: {}".format( id, layer ) )

        if self.data_post_process_fn:
            self.message( "Post processing function is {}".format( self.data_post_process_fn.__name__ ) )

        try:
            data = layer_info.layer.weight.grad.unsqueeze( 0 )
        except:
            self.error( "Current layer has no gradients" )
        else:
            title = "{} gradients".format( layer_info.id )
            self.display_bargraph( data, title, reduce_fn=self.data_post_process_fn )


    def do_set_post_process( self, args ):
        if args == "relu":
            fn = torch.nn.ReLU()
        elif args == "mean":
            fn = torch.mean     # pylint: disable=no-member
        elif args == "max":
            fn = torch.max      # pylint: disable=no-member
        elif args == "none" or args == "None":
            fn = None
        else:
            fn = self.load_from_global( args )
            if not fn:
                self.error( "Could not find function \"{}\"".format( args ) )
                return

        if not fn:
            self.message( "Removing post processing function" )
            self.data_post_process_fn = None
            return

        if fn and not callable( fn ):
            self.error( "Not a valid function" )
            return

        self.data_post_process_fn = fn
        if not hasattr( self.data_post_process_fn, "__name__" ):
            self.data_post_process_fn.__name__ = args
        self.message( "Post process function is {}".format( self.data_post_process_fn.__name__ ) )

    do_set_postp = do_set_post_process


    def do_assign( self, args ):
        args = args.split()

        def _set_name_and_check_in_use( default ):
            var = args[ 1 ] if len( args ) > 1 else default    
            if var in globals():
                self.error( "Variable \"{}\" already in use.".format( var ) )
                self.help( "please \"del {}\" first, if you want to reassign this var".format( var ) )
                return
            else:
                return var

        if not self.cur_model:
            self.error( "Please set a model in context first")
            return
        if not args:
            self.error( "No valid options provided" )
            return

        if args[ 0 ] == "layer":
            var = _set_name_and_check_in_use( "layer" )
            globals()[ var ] = self.cur_model.cur_layer.layer

        elif args[ 0 ] == "weight":
            var = _set_name_and_check_in_use( "weight" )
            globals()[ var ] = self.cur_model.cur_layer.layer.weight.data.detach().clone()

        elif args[ 0 ] == "out" or args[ 0 ] == "outsq":
            var = _set_name_and_check_in_use( "out" )
            self.cur_model.cur_layer.register_forward_hook()
            if image is None:
                globals()[ var ] = None
            else:
                _ = self.cur_model.model( image )
            if args[ 0 ] == "outsq":
                data = self.cur_model.cur_layer.data()
                globals()[ var ] = data.squeeze( 0 ) if data is not None else None
            else:
                globals()[ var ] = self.cur_model.cur_layer.data()
        else:
            self.error( "Invalid comand option \"{}\"".format( args[ 0 ] ) )


    def do_set_context( self, args ):
        model_name = args if args else "model"
        model = self.load_from_global( model_name )
        if model is None:
            self.error( "Could not find a model by name \"{}\"".format( model_name ) )
            return

        if not isinstance( model, nn.Module ):
            self.error( "{} is not a valid model" )
            return

        self.set_model( model_name, model )

        self.message( "Context now is \"{}\"".format( model_name ) )
        self.fig.set_window_title( model_name )
    
    do_set_ctx = do_set_context


    def do_resync( self, args ):
        if not args:
            self.error( "Please provide a model name" )
            return

        if args not in self.models:
            self.error( "Model \"{}\" not in context".format( args ) )
            return

        self.resync_model( args )
        

    ####################################################
    # Helper functions to debugger functionality go here
    ####################################################
    def expand_path( self, path ):
        """Expand environment variables and user home directory in path
        """
        if path is None:
            return None
        return os.path.expandvars( os.path.expanduser( path ) )


    def expand_config_paths( self ):
        """Expand environment variables in all path-related config attributes
        """
        path_attrs = [ 'image_path', 'checkpoint_path', 'dataset', 'train_path', 'val_path' ]
        for attr in path_attrs:
            if hasattr( self.config, attr ):
                value = getattr( self.config, attr )
                if value:
                    setattr( self.config, attr, self.expand_path( value ) )


    def load_first_image_from_dataset( self ):
        """Load the first image from the dataset if available
        """
        if self.dataset is None:
            return

        global image            
        self.message( "Loading first image from dataset...", end="" )
        try:
            self.dataset.next()
            image = self.dataset.load()
            self.message( "Done" )
        except Exception as e:
            self.message( "FAILED")


    def pre_process_rc_line( self, line ):
        """Process rc file line to handle quoted and unquoted paths
        Automatically adds quotes around unquoted paths in config assignments
        """
        # Match pattern: config.something = value
        match = re.match( r'^\s*(config\.\w+)\s*=\s*(.+)$', line )
        if not match:
            return line

        config_var = match.group( 1 )
        value = match.group( 2 ).strip()

        # Check if value is already quoted
        if (value.startswith( '"' ) and value.endswith( '"' )) or \
           (value.startswith( "'" ) and value.endswith( "'" )):
            return line
        else:
            return '{} = "{}"'.format( config_var, value )


    def exec_rc( self ):
        if not self.rc_lines:
            return
        self.message( "\nReading rc file...", end="" )

        self.stack.append( self.quiet )
        self.quiet = True
        num = 1
        while self.rc_lines:
            line = self.rc_lines.pop( 0 ).strip()
            self.message( "{}: {}".format( num, line ), end="" )
            num += 1
            if not line or "#" in line[ 0 ]:
                self.message()
                continue
            processed_line = self.pre_process_rc_line( line )
            self.onecmd( self.precmd( processed_line ) )
        self.message()
        self.expand_config_paths()
        self.quiet = self.stack.pop()
        self.message( "Done" )


    def load_and_execute_rc_file( self ):
        """
        Look for .pmdebugrc in current directory first, then in home directory,
        then in script directory. Load and execute the first one found.
        """
        rc_file = None
        if os.path.exists( ".pmdebugrc" ):
            rc_file = ".pmdebugrc"
        elif os.path.exists( os.path.expanduser( "~/.pmdebugrc" ) ):
            rc_file = os.path.expanduser( "~/.pmdebugrc" )
        else:
            script_dir = os.path.dirname( os.path.abspath( __file__ ) )
            rc_file = os.path.join( script_dir, ".pmdebugrc" )

        if rc_file:
            try:
                with open( rc_file ) as f:
                    self.rc_lines.extend( f )
            except OSError:
                pass
        self.exec_rc()


    def init_history( self, histfile ):
        try:
            readline.read_history_file( histfile )
        except FileNotFoundError:
            pass        
        readline.set_history_length( 2000 )
        readline.set_auto_history( True )


    def save_history( self, histfile ):
        self.message( "Saving history" )
        readline.write_history_file( histfile )


    def safe_mode( self ):
        os.environ[ "PYTORCH_ENABLE_MPS_FALLBACK" ] = "1"
        os.environ[ "OMP_NUM_THREADS" ] = "1"
        os.environ[ "MKL_NUM_THREADS" ] = "1"

        # Force MKL to use safe path
        os.environ[ "MKL_DEBUG_CPU_TYPE" ] = "5"
        
        torch.set_num_threads( 1 )
        torch.set_num_interop_threads( 1 )


    def _cmdloop( self, intro_header ):
        self.message( "Welcome to the debug shell!" )
        while True:
            try:
                self.allow_kbdint = True
                self.cmdloop( intro_header )
                self.allow_kbdint = False
                break
            except KeyboardInterrupt:
                self.message( "\n**Keyboard Interrupt" )
            except ( AttributeError, TypeError, NameError ) as e:
                self.error( e )
                traceback.print_exc()
            except ( RuntimeError, IndexError ):
                traceback.print_exc()

if __name__ == "__main__":
    # Parse command-line arguments
    parser = argparse.ArgumentParser( description='PyTorch Model Debug Shell' )
    parser.add_argument( '--safe-mode', action='store_true',
                        help='Enable safe mode (sets environment variables for CPU threading)' )
    args = parser.parse_args()

    config = Config()
    shell = Shell( config )

    # Call safe_mode() before doing anything if requested
    if args.safe_mode:
        shell.message( "Enabling safe mode..." )
        shell.safe_mode()

    shell.prompt = '>> '
    shell._cmdloop( "" )